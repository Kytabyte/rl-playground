{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from envs import ObservableEnv, env_gridworld\n",
    "import rl.mdp as mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slides Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Environment as torch tensor\n",
    "\n",
    "* Reward:\n",
    "    A 2D array of the rewards of $[s, a]$\n",
    "\n",
    "* Transition:\n",
    "    A 3D array of the probability of reaching state $s'$ under state $s$ and taking aciton $a$\n",
    "    \n",
    "* Value:\n",
    "    A 1D array of the expected value at state $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['PU', 'PF', 'RU', 'RF']\n",
    "actions = ['S', 'A']\n",
    "rewards = torch.tensor([[0, 0],\n",
    "                        [0, 0],\n",
    "                        [10, 10],\n",
    "                        [10, 10]], dtype=torch.float32)\n",
    "transitions = torch.tensor([[[1, 0, 0, 0], [0.5, 0.5, 0, 0]],\n",
    "                            [[0.5, 0., 0., 0.5], [0., 1., 0., 0.]],\n",
    "                            [[0.5, 0., 0.5, 0.], [0.5, 0.5, 0., 0.]],\n",
    "                            [[0., 0., 0.5, 0.5], [0., 1., 0., 0.]]], dtype=torch.float32)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ObservableEnv(states, actions, transitions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 90 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([31.5824, 38.6013, 44.0214, 54.1988]), tensor([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([0, 0, 10, 10], dtype=torch.float32)\n",
    "\n",
    "value_iteration = mdp.ValueIteration(env, value, gamma)\n",
    "value_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 2 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000,  4.5000, 14.5000, 19.0000]), tensor([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([0, 0, 10, 10], dtype=torch.float32)\n",
    "policy = torch.tensor([random.randint(0, 1) for _ in range(4)])\n",
    "\n",
    "policy_iteration = mdp.PolicyIteration(env, value, policy, gamma)\n",
    "policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 19 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([31.5848, 38.6037, 44.0239, 54.2013]), tensor([1, 0, 0, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.tensor([0, 0, 10, 10], dtype=torch.float32)\n",
    "policy = torch.tensor([random.randint(0, 1) for _ in range(4)])\n",
    "n_evals = 5\n",
    "\n",
    "modified_policy_iteration = mdp.ModifiedPolicyIteration(env, value, policy, gamma, n_evals)\n",
    "modified_policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigment 1 Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Environment\n",
    "\n",
    "Grid world layout:\n",
    "\n",
    "  \\---------------------  \n",
    "  |  0 |  1 |  2 |  3 |  \n",
    "  \\---------------------  \n",
    "  |  4 |  5 |  6 |  7 |  \n",
    "  \\---------------------  \n",
    "  |  8 |  9 | 10 | 11 |  \n",
    "  \\---------------------  \n",
    "  | 12 | 13 | 14 | 15 |  \n",
    "  \\---------------------  \n",
    "\n",
    "  Goal state: 15   \n",
    "  Bad state: 9  \n",
    "  End state: 16  \n",
    " \n",
    "$|S| = 17$  \n",
    "$|A| = 4$\n",
    "\n",
    "transitions: $|S| * |A| * |S'|$  \n",
    "rewards = $|S| * |A|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [str(i) for i in range(17)]\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "transitions = env_gridworld.transitions\n",
    "rewards = env_gridworld.rewards\n",
    "\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ObservableEnv(states, actions, transitions, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 738 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.9667e+01,  4.6420e+01,  5.4069e+01,  6.1717e+01,  4.0832e+01,\n",
       "          4.7916e+01,  6.2967e+01,  7.2633e+01,  4.1804e+01, -6.6523e+00,\n",
       "          7.3775e+01,  8.5318e+01,  5.5056e+01,  6.5748e+01,  8.5318e+01,\n",
       "          1.0000e+02,  1.1978e-11]),\n",
       " tensor([3, 3, 1, 1, 3, 3, 1, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.empty(len(states), dtype=torch.float32)\n",
    "\n",
    "value_iteration = mdp.ValueIteration(env, value, gamma)\n",
    "value_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 54 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([3.0285e+30, 3.0285e+30, 3.0285e+30, 3.0285e+30, 3.0285e+30, 3.0285e+30,\n",
       "         3.0285e+30, 3.0285e+30, 3.0285e+30, 3.0285e+30, 3.0285e+30, 3.0285e+30,\n",
       "         3.0285e+30, 3.0285e+30, 3.0285e+30, 1.0000e+02, 2.6930e-06]),\n",
       " tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 2, 3, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.empty(len(states), dtype=torch.float32)\n",
    "policy = torch.tensor([random.randint(0, len(actions) - 1) for _ in range(len(states))], dtype=torch.int64)\n",
    "\n",
    "policy_iteration = mdp.PolicyIteration(env, value, policy, gamma)\n",
    "policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 7 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 60.6326,  66.0390,  71.8062,  77.0930,  59.8195,  65.1846,  77.8315,\n",
       "          84.1415,  58.0956,   7.9886,  84.8673,  91.7816,  69.4968,  76.8099,\n",
       "          91.7816, 100.0000,   0.0000]),\n",
       " tensor([3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.empty(len(states))\n",
    "policy = torch.tensor([random.randint(0, len(actions) - 1) for _ in range(len(states))])\n",
    "n_evals = 5\n",
    "\n",
    "modified_policy_iteration = mdp.ModifiedPolicyIteration(env, value, policy, gamma, n_evals)\n",
    "modified_policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.env_gridworld import EnvGridWorld\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_evaluation(env, gamma=0.95):\n",
    "    policy = lambda state: random.randint(0, env.n_actions() - 1)\n",
    "    value = torch.zeros(env.n_states())\n",
    "    \n",
    "    counter = collections.Counter()\n",
    "    dones = set()\n",
    "    \n",
    "    for i in itertools.count(1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        factor = 1\n",
    "        G = 0\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            counter[state] += 1\n",
    "            alpha = 1 / counter[state]\n",
    "            G += factor * reward\n",
    "            next_value = value[state] + alpha * (G - value[state])\n",
    "            factor *= gamma\n",
    "            \n",
    "            if abs(next_value - value[state]) < 1e-3:\n",
    "                dones.add(state)\n",
    "            value[state] = next_value\n",
    "            if len(dones) == env.n_states() - 1:\n",
    "                print(f'converged in {i} rounds')\n",
    "                return value\n",
    "            state = next_state\n",
    "        if i % 10000 == 0:\n",
    "            print(f'{i} rounds has been processed, {len(dones)} are converged')\n",
    "        \n",
    "        if i >= 100000:\n",
    "            print(value)\n",
    "            print(dones)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC Evaluation on maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged in 771 rounds\n",
      "tensor([-140.9322, -182.0999, -218.0852, -238.9838, -195.9595, -232.1794,\n",
      "        -245.0768, -253.5311, -249.8367, -325.9745, -273.7172, -256.5232,\n",
      "        -248.8553, -287.7272, -286.9494, -182.4487,    0.0000])\n",
      "CPU times: user 4.87 s, sys: 48 Âµs, total: 4.87 s\n",
      "Wall time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "env = EnvGridWorld()\n",
    "value = MC_evaluation(env,0.999)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_evaluation(env, gamma=0.95):\n",
    "    policy = lambda state: random.randint(0, env.n_actions() - 1)\n",
    "    value = torch.zeros(env.n_states())\n",
    "    \n",
    "    counter = collections.Counter()\n",
    "    dones = set()\n",
    "    \n",
    "    for i in itertools.count(1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            counter[state] += 1\n",
    "            next_value = value[state] + (reward + gamma * value[next_state] - value[state]) / counter[state]\n",
    "            if abs(next_value - value[state]) < 1e-3:\n",
    "                dones.add(state)\n",
    "            value[state] = next_value\n",
    "            if len(dones) == env.n_states() - 1:\n",
    "                print(f'converged in {i} rounds')\n",
    "                return value\n",
    "            state = next_state\n",
    "        if i % 10000 == 0:\n",
    "            print(f'{i} rounds has been processed, {len(dones)} are converged')\n",
    "        \n",
    "        if i >= 100000:\n",
    "            print(value)\n",
    "            print(dones)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD evaluation on maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged in 1710 rounds\n",
      "tensor([ -21.5537,  -21.4420,  -12.0640,   -4.1311,  -33.1369,  -40.2014,\n",
      "         -15.1712,    2.4095,  -52.3734, -103.5385,  -17.7937,   26.8511,\n",
      "         -38.6718,  -40.2930,   13.0381,  100.0000,    0.0000])\n",
      "CPU times: user 12.7 s, sys: 1.1 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "env = EnvGridWorld()\n",
    "value = TD_evaluation(env)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD control (a.k.a. Q-learning)\n",
    "\n",
    "Q-learning is an alias of TD control algorithm, as we are calling the state-action function as q-function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning on maze\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, gamma=0.95):\n",
    "    policy = lambda value, state: torch.argmax(value[state]).item() if random.random() < 0.9 else random.randint(0, env.n_actions() - 1)\n",
    "    value = torch.zeros(env.n_states(), env.n_actions())\n",
    "    \n",
    "    counter = collections.Counter()\n",
    "    dones = set()\n",
    "    \n",
    "    for i in itertools.count(1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy(value, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            counter[(state, action)] += 1\n",
    "            alpha = 1 / counter[(state, action)]\n",
    "            next_value = value[state, action] + alpha * (reward + gamma * torch.max(value[next_state]).item() - value[state, action])\n",
    "            if abs(next_value - value[state, action]) < 1e-3:\n",
    "                dones.add((state, action))\n",
    "            value[state, action] = next_value\n",
    "            if len(dones) == (env.n_states() - 1) * env.n_actions():\n",
    "                print(f'converged in {i} rounds')\n",
    "                return value\n",
    "            state = next_state\n",
    "        if i % 10000 == 0:\n",
    "            print(f'{i} rounds has been processed, {len(dones)} are converged')\n",
    "        \n",
    "        if i >= 50000:\n",
    "            print(value)\n",
    "            print(dones)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 rounds has been processed, 29 are converged\n",
      "20000 rounds has been processed, 31 are converged\n",
      "30000 rounds has been processed, 33 are converged\n",
      "40000 rounds has been processed, 34 are converged\n",
      "50000 rounds has been processed, 35 are converged\n",
      "tensor([[ 4.5484e+01,  3.7568e+01,  4.4004e+01,  5.2755e+01],\n",
      "        [ 5.3951e+01,  5.4434e+01,  4.8159e+01,  6.0635e+01],\n",
      "        [ 6.1952e+01,  6.8369e+01,  5.6413e+01,  6.4404e+01],\n",
      "        [ 6.4771e+01,  7.4762e+01,  6.2408e+01,  4.6644e+01],\n",
      "        [ 4.0088e+01,  4.3283e+01,  4.0675e+01,  4.9019e+01],\n",
      "        [ 5.1920e+01,  1.2463e+01,  3.9142e+01,  6.0513e+01],\n",
      "        [ 6.2222e+01,  7.5964e+01,  5.7032e+01,  7.5491e+01],\n",
      "        [ 6.8240e+01,  8.3358e+01,  7.0596e+01,  7.6547e+01],\n",
      "        [ 3.0953e+01,  5.1495e+01,  3.6134e+01,  9.4052e+00],\n",
      "        [-1.3430e+01,  8.7110e-02, -2.2170e+01,  5.3969e+00],\n",
      "        [ 6.3412e+01,  7.5727e+01,  1.6713e+01,  8.4333e+01],\n",
      "        [ 7.8293e+01,  9.1660e+01,  7.9791e+01,  8.5845e+01],\n",
      "        [ 4.5421e+01,  5.6686e+01,  5.3726e+01,  6.6789e+01],\n",
      "        [ 1.7120e+01,  7.0241e+01,  5.4685e+01,  7.6030e+01],\n",
      "        [ 7.7512e+01,  8.4980e+01,  7.3258e+01,  9.1485e+01],\n",
      "        [ 1.0000e+02,  1.0000e+02,  1.0000e+02,  1.0000e+02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "{(1, 3), (12, 1), (11, 2), (2, 1), (15, 1), (6, 2), (5, 1), (0, 3), (10, 3), (7, 2), (1, 2), (15, 0), (11, 1), (5, 0), (2, 2), (12, 3), (3, 2), (8, 2), (7, 1), (15, 3), (9, 3), (11, 0), (2, 3), (10, 1), (1, 0), (14, 2), (5, 3), (0, 1), (8, 3), (15, 2), (6, 1), (3, 1), (11, 3), (4, 3), (14, 3)}\n",
      "CPU times: user 4min 31s, sys: 11.1 s, total: 4min 42s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "env = EnvGridWorld()\n",
    "value = q_learning(env)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
